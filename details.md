# LLM4RD: LLM for Reward Design 工作介绍

## 任务背景：

奖励函数在RL中是直接决定学习效果的一环，而人为设计奖励函数需要大量的先验知识。
LLM对于任务的理解和翻译为代码语言的能力很强，同时能大量多次生成，有助于迭代设计和验证奖励函数的优劣。

### 相关工作：

1. `Auto MC reward : Automated Dense Reward Design with Large Language Models for Minecraft`
- 奖励的极度稀疏性和决策空间的巨大复杂性给强化学习方法带来了重大挑战，这篇文章基于LLM自动设计密集奖励函数强化学习效率
- 算法过程：
  - `Reward Designer`先通过使用预定义的观察输入编写可执行 Python 函数来设计奖励函数
  - `Reward Critic`负责验证代码，检查代码是否没有语法和语义错误，Trajectory Analyzer根据收集的轨迹总结可能的故障原因并提供细化建议
  - 下一轮，`Reward Designer`将根据反馈（是人工输入的反馈）进一步细化和迭代密集奖励功能
  - 基于智能体的历史行动轨迹和成功失败信号，LLM可以自动设计和细化相应的辅助奖励，有效克服Mincraft中奖励稀疏的挑战

2. `Eureka: Human-Level Reward Design via Coding Large Language Models `
- EUREKA 利用LLM卓越的零样本泛化、代码编写和上下文改进能力，对奖励代码进行渐进优化。没有任何特定任务的提示或预定义的奖励模板，EUREKA 生成的奖励函数优于专家设计的奖励。
- 算法过程：
  - 环境作为上下文，能够零样本生成可执行奖励
  - 以迭代方式提出并改进奖励候选项的进化搜索：EUREKA 在获得可执行的奖励函数后，执行上下文奖励变异，根据文本反馈从现有奖励函数生成新的改进奖励函数。
    - 新的迭代将上一个迭代中表现最佳的奖励作为上下文，并从 LLM 中生成更多的独立同分布奖励输出，数量为 K。最后执行多次随机重新启动以寻找更好的全局解决方案。
  - 细粒度奖励改进的奖励反思
    - EUREKA 可以以多种不同的方式进行自由修改奖励函数，比如（1）更改现有奖励部分的参数，（2）更改现有奖励部分的函数形式，和（3）引入新的奖励部分
    - EUREKA 的通用性也使一种新的无梯度上下文学习方法能够从人类反馈(RLHF)中进行强化学习，容易地结合人类输入来提高生成奖励的质量
    - 由于 EUREKA 奖励函数要求在奖励程序中公开其各个组成部分，作者在训练的不同策略检查点中追踪奖励组成的数值。

## 任务方案：

### LLM提示部分：
1. `in-context learning`：将一类特定的运动项目的规则，以及原始仿真环境（如SMPLOlympics）能获取的observation给LLM，可以零样本生成奖励函数
  1.  LLM 直接返回可执行的 Python 代码，除环境上下文外的代码提示仅包括格式化提示，例如将奖励中的各个组件作为字典输出
  2. 先得到运动项目大类的reward初版output，再作为in-context用以辅助该运动的细分变种进行针对性设计
2. 迭代优化：首先输入某一大类运动项目的基本目标（比如拳击，目标是deliver blows to opponents or knock out their opponents to win the match），生成第一轮针对大类的reward；将其连同细分运动项目的细则和要点（比如泰式拳击，要点是……），一起作为新的上下文输入，获得第二轮的细分运动reward；接下来的K-2轮均是将reward结果放到仿真环境中验证，并将结果作为新增input以迭代生成reward
3. LLM帮助agent模拟人类的实践总结能力：LLM从仿真环境中的输出结果，进行总结分析，推理每一轮结果需要改进的点并对python代码进行改进。

### 仿真环境部分：
1. 基于SMPLOlympics这篇工作，其基于Mujoco物理引擎 + isaac gym强化学习环境，集成了各种常见运动项目的环境仿真；其工作本身也有一些benchmark算法的验证，如AMP, PULSE, PPO
2. 运动任务的选取：对抗性运动，因为规则更复杂，agent需要通过规则学习策略，LLM零样本泛化、代码编写与上下文改进的优势体现更显著
3. 暂时是设计相同策略的agent进行对抗，也可以设计不同策略的agent对抗。

### 当前部分实验结果：
1. LLM输出的结果比较靠谱合理，能考虑到基本的运动要素以及转化为基本的奖励项，给出的代码形式风格也和手动设计的一致；
2. 使用拳击项目进行验证的时候会出现两个agent没有接触的情形，LLM有时候会没有考虑接触reward项
