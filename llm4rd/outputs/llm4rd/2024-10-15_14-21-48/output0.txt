@torch.jit.script
def compute_reward(self_root_state, self_body_pos, oppo_root_state, oppo_body_pos, oppo_body_rot,
                 oppo_dof_pos, oppo_dof_vel, self_contact_norm, oppo_contact_norm,
                 hand_ids, target_ids):
    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]

    reward = 0.0
    rewards_dict = {}

    # distance reward for delivering effective blows
    dist_reward = torch.zeros_like(self_root_state[:, 0])
    tar_hand_pos_diff = self_body_pos[:, hand_ids, :].unsqueeze(-2) - oppo_body_pos[:, target_ids, :].unsqueeze(-3).repeat((1, self_hand_pos.shape[1], 1, 1))
    local_tar_hand_pos_diff = torch_utils.quat_rotate(heading_rot.unsqueeze(-2).repeat((1, tar_hand_pos_diff.shape[0] // heading_rot.shape[0], 1)), tar_hand_pos_diff)
    dist_reward += (local_tar_hand_pos_diff.norm(dim=-1) > 0.5).float() * 0.05

    # knock out reward
    knock_out_reward = torch.zeros_like(self_root_state[:, 0])
    if oppo_contact_norm[0, 0] > 10:
        knock_out_reward += 100.0

    # time penalty
    time_penalty = self_root_state[:, 6]
    rewards_dict['dist_reward'] = dist_reward * (time_penalty < 300).float()
    rewards_dict['knock_out_reward'] = knock_out_reward * (time_penalty < 300).float() 
    reward = dist_reward + knock_out_reward - time_penalty
    return reward, rewards_dict

@torch.jit.script
def compute_reward(self_contact_norm, oppo_dof_pos):
    # type: (Tensor, Tensor) -> Tuple[Tensor, Dict[str, Tensor]]

    self_contact = self_contact_norm > 0.5
    knock_out = torch.sum(oppo_dof_pos[:, :3] < -0.8).float()

    reward = torch.where(self_contact & knock_out, 10.0, torch.where(self_contact, 1.0, torch.zeros_like(knock_out)))
    component_reward = {'knockout': knock_out}
    
    return reward, component_reward

@torch.jit.script
def compute_reward(self_root_state, self_body_pos, oppo_root_state, oppo_body_pos, oppo_dof_pos,
                  self_contact_norm, oppo_contact_norm, hand_ids):
    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) -> Tuple[Tensor, Dict[str, Tensor]]

    root_info = self_root_state[:, 0:3] - oppo_root_state[:, 0:3]
    heading_rot = torch_utils.calc_heading_quat_inv(self_root_state[:, 3:7])
    local_root_pos_diff = torch_utils.quat_rotate(heading_rot, root_info)
    local_oppo_vel = torch_utils.quat_rotate(heading_rot, oppo_root_state[:, 10:13])

    # hand positions
    self_hand_pos = self_body_pos[:, hand_ids, :].unsqueeze(-2)
    oppo_target_pos = oppo_body_pos.unsqueeze(-3).repeat((1, self_hand_pos.shape[1], 1, 1))
    global_target_hand_pos_diff = (oppo_target_pos - self_hand_pos).view(-1, 3)
    flat_heading_rot = heading_rot.unsqueeze(-2).\
        repeat((1, global_target_hand_pos_diff.shape[0] // heading_rot.shape[0], 1))
    local_target_hand_pos_diff = torch_utils.quat_rotate(flat_heading_rot.view(-1, 4), global_target_hand_pos_diff)
    local_target_hand_pos_diff = local_target_hand_pos_diff.view(flat_heading_rot.shape[0], -1)

    # calculate the distance and rotation between hands
    dist_diff = torch.norm(local_target_hand_pos_diff, dim=-1).unsqueeze(-1) * 100.0

    hand_rotation_diff = torch_utils.quat_to_tan_norm(torch_utils.quat_mul(heading_rot, oppo_root_state[:, 3:7])) - \
                       torch_utils.quat_to_tan_norm(self_root_state[:, 3:7])

    # calculate the time left
    time_left = self_contact_norm[0] / 100.0

    # create a dictionary of individual reward components
    dist_reward = torch.sigmoid(dist_diff) * time_left
    rotation_reward = hand_rotation_diff * torch.sigmoid(-dist_diff)
    contact_reward = oppo_contact_norm[0].unsqueeze(-1)

    total_reward = dist_reward + rotation_reward + contact_reward

    return total_reward, {
        'distance': dist_reward,
        'rotation': rotation_reward,
        'contact': contact_reward
    }

